{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9def82",
   "metadata": {},
   "source": [
    "机器翻译中的输入序列和输出序列都是长度可变的。 为了解决这类问题，设计了一个通用的”编码器－解码器“架构。 \n",
    "本节，我们将使用两个循环神经网络的编码器和解码器，并将其应用于序列到序列（sequence to sequence，seq2seq）类的学习任务 (Cho et al., 2014, Sutskever et al., 2014)。\n",
    "\n",
    "遵循编码器－解码器架构的设计原则， 循环神经网络编码器使用长度可变的序列作为输入，将其转换为固定形状的隐状态。 换言之，输入序列的信息被编码到循环神经网络编码器的隐状态中。 \n",
    "为了连续生成输出序列的词元，独立的循环神经网络解码器是基于输入序列的编码信息和输出序列已经看见的或者生成的词元来预测下一个词元。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d50802",
   "metadata": {},
   "source": [
    "特定的“<eos>”表示序列结束词元。 一旦输出序列生成此词元，模型就会停止预测。 \n",
    "在循环神经网络解码器的初始化时间步，有两个特定的设计决定： \n",
    "    首先，特定的“<bos>”表示序列开始词元，它是解码器的输入序列的第一个词元。 \n",
    "    其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。 \n",
    "例如，在 (Sutskever et al., 2014)的设计中， 正是基于这种设计将输入序列的编码信息送入到解码器中来生成输出序列的。 \n",
    "在其他一些设计中 (Cho et al., 2014)，编码器最终的隐状态在每一个时间步都作为解码器的输入序列的一部分。 \n",
    "类似于 8.3节中语言模型的训练，可以允许标签成为原始的输出序列，从源序列词元“<bos>”“Ils”“regardent”“.” 到新序列词元 “Ils”“regardent”“.”“<eos>”来移动预测的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563ca8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3a38e8",
   "metadata": {},
   "source": [
    "到目前为止，我们使用的是一个单向循环神经网络来设计编码器， 其中隐状态只依赖于输入子序列， 这个子序列是由输入序列的开始位置到隐状态所在的时间步的位置 （包括隐状态所在的时间步）组成。 我们也可以使用双向循环神经网络构造编码器， 其中隐状态依赖于两个输入子序列， 两个子序列是由隐状态所在的时间步的位置之前的序列和之后的序列 （包括隐状态所在的时间步）， 因此隐状态对整个序列的信息都进行了编码。\n",
    "\n",
    "现在，让我们实现循环神经网络编码器。 注意，我们使用了嵌入层（embedding layer） 来获得输入序列中每个词元的特征向量。 嵌入层的权重是一个矩阵， 其行数等于输入词表的大小（vocab_size）， 其列数等于特征向量的维（embed_size）。 对于任意输入词元的索引i， 嵌入层获取权重矩阵的第i行（从0开始）以返回其特征向量。 另外，本文选择了一个多层门控循环单元来实现编码器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f7cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络编码器\"\"\"\n",
    "\n",
    "    # vocab_size：词汇表大小，决定嵌入层的输入维度\n",
    "    # embed_size：词嵌入维度，每个词元表示为多少维的向量\n",
    "    # num_hiddens：GRU隐藏单元数量，决定隐藏状态维度\n",
    "    # num_layers：GRU层数，增加层数可提高模型捕获复杂模式的能力\n",
    "    # dropout：防止过拟合的丢弃率，默认为0表示不使用dropout\n",
    "    # **kwargs：其他可能传递给基类的参数\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        # 嵌入层\n",
    "        # 创建嵌入层，将词元索引转换为密集向量\n",
    "        # 词元是离散的符号，需要转换为连续的向量才能被神经网络处理\n",
    "        # 嵌入层学习词元的分布式表示，可以捕获词元之间的语义关系\n",
    "        # 降低输入维度，使用密集向量而非独热编码，减少参数数量\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # 使用门控循环单元作为循环神经网络\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X)\n",
    "        # 在循环神经网络模型中，第一个轴对应于时间步\n",
    "        # 将张量维度从(batch_size, num_steps, embed_size)转换为(num_steps, batch_size, embed_size)\n",
    "        # 这种排列使得模型按时间步处理序列，而不是按批次处理\n",
    "        X = X.permute(1, 0, 2)\n",
    "        # 如果未提及状态，则默认为0\n",
    "        output, state = self.rnn(X)\n",
    "        # output的形状:(num_steps,batch_size,num_hiddens)\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc2861",
   "metadata": {},
   "source": [
    "然后可以实例化上述编码器的实现： 我们使用一个两层门控循环单元编码器，其隐藏单元数为16。 \n",
    "给定一小批量的输入序列X（批量大小为4，时间步为7）。 在完成所有时间步后， 最后一层的隐状态的输出是一个张量（output由编码器的循环层返回）， 其形状为（时间步数，批量大小，隐藏单元数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e66312b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "encoder.eval()\n",
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe510e6",
   "metadata": {},
   "source": [
    "由于这里使用的是门控循环单元， 所以在最后一个时间步的多层隐状态的形状是 （隐藏层的数量，批量大小，隐藏单元的数量）。 如果使用长短期记忆网络，state中还将包含记忆单元信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867638e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13477d6a",
   "metadata": {},
   "source": [
    "实现解码器时， 我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。 \n",
    "这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。 \n",
    "为了进一步包含经过编码的输入序列的信息， 上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。 为了预测输出词元的概率分布， 在循环神经网络解码器的最后一层使用全连接层来变换隐状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0f7e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络解码器\"\"\"\n",
    "    # vocab_size：目标语言词汇表大小，决定嵌入层输入维度和输出层维度\n",
    "    # embed_size：词嵌入维度，目标序列词元的向量表示维度\n",
    "    # num_hiddens：GRU隐藏单元数量，需要与编码器保持一致\n",
    "    # num_layers：GRU层数，通常与编码器层数匹配\n",
    "    # dropout：防止过拟合的丢弃率\n",
    "    # **kwargs：传递给基类的其他参数\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "\n",
    "        # 同样使用嵌入层,将目标词元索引转换为密集向量表示\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "\n",
    "        # 添加全连接层，将隐藏状态映射到词汇表大小的输出\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        # 时间步为第一维度\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "\n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        # 提取最后一层的隐状态作为上下文向量\n",
    "        # 之所以要进行重复,是为了每个解码时间步都能使用相同的上下文信息\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "\n",
    "        # 将输入X和上下文向量连接在一起\n",
    "        # 连接的目的是将编码器的上下文信息传递给解码器\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "\n",
    "        # 返回所有时间步的输出和最终隐状态\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # output的形状:(batch_size,num_steps,vocab_size)\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b14bb",
   "metadata": {},
   "source": [
    "同样,这里使用相同的超参数来实例化解码器\n",
    "解码器的输出形状变为（批量大小，时间步数，词表大小）， 其中张量的最后一个维度存储预测的词元分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea5e19f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "decoder.eval()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6098e59f",
   "metadata": {},
   "source": [
    "在每个时间步，解码器预测了输出词元的概率分布。 \n",
    "类似于语言模型，可以使用softmax来获得分布，并通过计算交叉熵损失函数来进行优化。 \n",
    "特定的填充词元被添加到序列的末尾，因此不同长度的序列可以以相同形状的小批量加载。 \n",
    "但是，我们应该将填充词元的预测排除在损失函数的计算之外。\n",
    "\n",
    "为此，我们可以使用下面的sequence_mask函数 通过零值化屏蔽不相关的项， 以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。 例如，如果两个序列的有效长度（不包括填充词元）分别为1和2， 则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d0ab7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@save\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "\n",
    "    # 依据给定的有效长度进行操作\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "\n",
    "    # 取反，标记出无效位置 \n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "sequence_mask(X, torch.tensor([1, 2]))  # 表示序列1的有效长度是1，序列2的有效长度是2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f483fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.],\n",
       "         [-1., -1., -1., -1.]],\n",
       "\n",
       "        [[ 1.,  1.,  1.,  1.],\n",
       "         [ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2, 3, 4)\n",
    "sequence_mask(X, torch.tensor([1, 2]), value=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d789e",
   "metadata": {},
   "source": [
    "现在，我们可以通过扩展softmax交叉熵损失函数来遮蔽不相关的预测。 最初，所有预测词元的掩码都设置为1。 一旦给定了有效长度，与填充词元对应的掩码将被设置为0。 最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a8d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失函数\"\"\"\n",
    "    # pred的形状：(batch_size,num_steps,vocab_size)\n",
    "    # label的形状：(batch_size,num_steps)\n",
    "    # valid_len的形状：(batch_size,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        # 首先创建和标签形状相同的全1权重张量\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "\n",
    "        # 这确保损失不会立即被求平均或求和，而是保持每个元素的独立损失值\n",
    "        self.reduction='none'\n",
    "\n",
    "        # 计算每个位置的原始交叉熵损失\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label)\n",
    "        \n",
    "        # 乘上权重掩码，使得填充位置的损失为0\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055129f",
   "metadata": {},
   "source": [
    "我们可以创建三个相同的序列来进行代码健全性检查， 然后分别指定这些序列的有效长度为4、2和0。 结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3f50c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),\n",
    "     torch.tensor([4, 2, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127dd2a",
   "metadata": {},
   "source": [
    "在下面的循环训练过程中，特定的序列开始词元（“<bos>”）和 原始的输出序列（不包括序列结束词元“<eos>”） 拼接在一起作为解码器的输入。 这被称为强制教学（teacher forcing）， 因为原始的输出序列（词元的标签）被送入解码器。 或者，将来自上一个时间步的预测得到的词元作为解码器的当前输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef6e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                     xlim=[10, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # 训练损失总和，词元数量\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                          device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()      # 损失函数的标量进行“反向传播”\n",
    "            d2l.grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "        f'tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851370f",
   "metadata": {},
   "source": [
    "现在，在机器翻译数据集上，我们可以 创建和训练一个循环神经网络“编码器－解码器”模型用于序列到序列的学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91bccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, device = 0.005, 300, d2l.try_gpu()\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e74a8",
   "metadata": {},
   "source": [
    "为了采用一个接着一个词元的方式预测输出序列， 每个解码器当前时间步的输入都将来自于前一时间步的预测词元。 与训练类似，序列开始词元（“<bos>”） 在初始时间步被输入到解码器中。\n",
    "当输出序列的预测遇到序列结束词元（“<eos>”）时，预测就结束了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9008571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    device, save_attention_weights=False):\n",
    "    \"\"\"序列到序列模型的预测\"\"\"\n",
    "    # 在预测时将net设置为评估模式\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n",
    "        src_vocab['<eos>']]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # 添加批量轴\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # 添加批量轴\n",
    "    dec_X = torch.unsqueeze(torch.tensor(\n",
    "        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # 保存注意力权重（稍后讨论）\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        # 一旦序列结束词元被预测，输出序列的生成就完成了\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17451a92",
   "metadata": {},
   "source": [
    "我们可以通过与真实的标签序列进行比较来评估预测序列\n",
    "\n",
    "我们将BLEU定义为：\n",
    "$$\n",
    "\\exp \\left( \\min \\left( 0, 1 - \\frac{\\text{len}_{\\text{label}}}{\\text{len}_{\\text{pred}}} \\right) \\right) \\prod_{n=1}^{k} p_n^{1/2^n},\n",
    "$$\n",
    "其中 $\\text{len}_{\\text{label}}$ 表示标签序列中的词元数和 $\\text{len}_{\\text{pred}}$ 表示预测序列中的词元数， k 是用于匹配的最长的 n 元语法。另外，用 p_n 表示 n 元语法的精确度，它是两个数量的比值：第一个是预测序列与标签序列中匹配的 n 元语法的数量，第二个是预测序列中 n 元语法的数量的比率。具体地说，给定标签序列 A, B, C, D, E, F 和预测序列 A, B, B, C, D ，我们有 $p_1 = 4/5$、 $p_2 = 3/4$、$p_3 = 1/3$ 和 $p_4 = 0$。\n",
    "\n",
    "根据BLEU的定义，当预测序列与标签序列完全相同时，BLEU为1。此外，由于 n 元语法越长则匹配难度越大，所以BLEU为更长的 n 元语法的精确度分配更大的权重。具体来说，当 $p_n$ 固定时，$p_n^{1/2^n}$ 会随着 n 的增长而增加（原始论文使用 $p_n^{1/n}$ ）。而且，由于预测的序列越短获得的 $p_n$ 值越高，所以乘法项之前的系数用于惩罚较短的预测序列。例如，当 k = 2 时，给定标签序列 A, B, C, D, E, F 和预测序列 A, B ，尽管 p_1 = p_2 = 1 ，惩罚因子 $\\exp(1 - 6/2) \\approx 0.14 $ 会降低BLEU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(pred_seq, label_seq, k):  #@save\n",
    "    \"\"\"计算BLEU\"\"\"\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, k + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1cc792",
   "metadata": {},
   "source": [
    "最后，利用训练好的循环神经网络“编码器－解码器”模型， 将几个英语句子翻译成法语，并计算BLEU的最终结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, attention_weight_seq = predict_seq2seq(\n",
    "        net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
