{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6382cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a19bf",
   "metadata": {},
   "source": [
    "生成数据集\n",
    "\n",
    "通过一个带有噪声的线性模型来生成数据\n",
    "其中w代表权重向量，b代表偏置项，num_examples代表要生成的样本数量\n",
    "\n",
    "torch.normal() 函数用于生成服从正态分布（高斯分布）的随机数。\n",
    "y.reshape((-1, 1))   -1是一个特殊值，意味着让pytorch自动计算该维度的大小以保持原数总数不变\n",
    "\n",
    "调用下面编写的函数会返回一个特征矩阵和一个标签向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92943677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))  # 通过正态分布，生成特征矩阵\n",
    "    y = torch.matmul(X, w) + b  # 通过matmul矩阵乘法，生成标签\n",
    "    y += torch.normal(0, 0.01, y.shape)  # 添加噪声\n",
    "    return X, y.reshape((-1, 1))  # 通过reshape来确保输出标签是列向量形式\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "#因为传入的w是二维的，也就是一个样本点的大小是二维的(有两个特征)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818655c0",
   "metadata": {},
   "source": [
    "读取数据集\n",
    "\n",
    "该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为batch_size的小批量。 每个小批量包含一组特征和标签。\n",
    "\n",
    "特征矩阵，形状为(样本数量，特征维度)\n",
    "标签向量，形状为(样本数量，1)\n",
    "\n",
    "其中features[batch_indices] 是按照索引列表进行取值的操作，这在 PyTorch 中被称为索引选择或高级索引\n",
    "features[batch_indices] 会返回一个新的张量，其中包含了 features 中对应这些索引位置的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88a5327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))  # 创建索引列表\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)  # 随机打乱样本顺序\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]  # 惰性计算，只在需要时生成批次数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8bd34f",
   "metadata": {},
   "source": [
    "初始化模型参数，定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c619999",
   "metadata": {},
   "source": [
    "定义优化算法\n",
    "\n",
    "parms 代表需要优化的参数列表\n",
    "lr 代表学习率\n",
    "batch_size 代表小批量的大小，用于归一化\n",
    "\n",
    "with torch.no_grad() 是一个上下文管理器(context manager)，在这个上下文中执行的操作不会被记录用于反向传播\n",
    "具体来说，会禁用梯度计算（临时关闭自动求导机制，不会小号额外的内存来存储计算历史）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4725fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a69e8",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82d5286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.034151\n",
      "epoch 2, loss 0.000120\n",
      "epoch 3, loss 0.000046\n"
     ]
    }
   ],
   "source": [
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"定义线性回归模型\"\"\"\n",
    "    return torch.matmul(X, w) + b\n",
    "\n",
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "# 超参数设置\n",
    "batch_size = 10\n",
    "\n",
    "# 读取数据集\n",
    "# for X, y in data_iter(batch_size, features, labels):\n",
    "#     print(X, '\\n', y)\n",
    "#     break\n",
    "\n",
    "lr = 0.03\n",
    "num_epochs = 3  # 训练轮数\n",
    "net = linreg  # 模型函数\n",
    "loss = squared_loss  # 损失函数，这里使用均方误差\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):  # 读取小批量数据\n",
    "        l = loss(net(X, w, b), y)  # X和y的小批量损失，前向传播\n",
    "        # 因为l形状是(batch_size,1)，而不是一个标量。将l中的所有元素加到一起，并以此计算关于[w,b]的梯度\n",
    "        l.sum().backward() # 反向传播，计算梯度\n",
    "        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n",
    "    with torch.no_grad():  # 评估当前训练效果，此时不需要记录操作\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d152c163",
   "metadata": {},
   "source": [
    "因为是自行生成的数据，因此可以直接进行比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8901dab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差: tensor([ 0.0007, -0.0003], grad_fn=<SubBackward0>)\n",
      "b的估计误差: tensor([-8.3447e-05], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
